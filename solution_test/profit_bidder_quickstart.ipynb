{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "profit_bidder_quickstart",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# License"
      ],
      "metadata": {
        "id": "J9_0gs_do2UH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blDi_vpj9lsV"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#      https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Run in Colab](https://colab.research.google.com/github/google/profit-bidder/blob/main/solution_test/profit_bidder_quickstart.ipynb)"
      ],
      "metadata": {
        "id": "W2JWjUOEr5j7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "The current notebook acts as a quick startup guide to make you understand the different steps involved in the solution. Unlike the production pipeline that you can set up using the complete solution, the notebook runs through all the steps in one place using synthesized test data. Please note that you will **not be able to test the final step** because of fake synthesized data.\n",
        "\n",
        "## Scope of this notebook\n",
        "### Dataset\n",
        "We provide synthesized data sets in the gitrepo that you will clone and use in the notebook. There are three csv files:\n",
        "* p_Campaign_43939335402485897.csv\n",
        "* p_Conversion_43939335402485897.csv\n",
        "* client_profit.csv\n",
        "\n",
        "In addition, we also provide the schema for the above files in json format which you will use in the notebook to create the tables in the BigQuery.\n",
        "\n",
        "### Objective\n",
        " To help you be conversant on the following:\n",
        "1. Setup your environment (install the libraries, initialize the variables, authenticate to Google Cloud, etc.)\n",
        "1. Create a service account and two BigQuery datasets\n",
        "1. Transform the data, create batches of the data, and push the data through a REST API call to CM360\n",
        "\n",
        "### Costs\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "* [BigQuery](https://cloud.google.com/bigquery)\n",
        "\n",
        "Use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
        "\n",
        "## Before you begin\n",
        "For this reference guide, you need a [Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).\n",
        "\n",
        "You can create a new one, or select a project you already created.\n",
        "The following steps are required, regardless where you are running your notebook (local or in Cloud AI Platform Notebook).\n",
        "* [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "* [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project). \n",
        "* (When using non-Google Cloud local envirionments)Install Google Cloud SDK [Google Cloud SDK](https://cloud.google.com/sdk/)\n",
        "\n",
        "### Mandatory variables\n",
        "You must set the below variables:\n",
        "* PB_GCP_PROJECT to [Your Google Cloud Project]\n",
        "* PB_GCP_APPLICATION_CREDENTIALS to [Full path with the file name to the Service Account json file, if you chose to use Service Account to authenticate to Google Cloud]"
      ],
      "metadata": {
        "id": "uygJvNun997m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup environment"
      ],
      "metadata": {
        "id": "Ar5KocE3-E8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *PIP install appropriate packages*"
      ],
      "metadata": {
        "id": "UqAA1lv7-IZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install google-cloud-storage # for Storage Account\n",
        "%pip install google-cloud # for cloud sdk\n",
        "%pip install google-cloud-bigquery # for BigQuery\n",
        "%pip install google-cloud-bigquery-storage # for BigQuery Storage client\n",
        "%pip install google-api-python-client # for Key management\n",
        "%pip install oauth2client # for Key management"
      ],
      "metadata": {
        "id": "7BrPbpj1-Fdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Initialize all the variables*"
      ],
      "metadata": {
        "id": "YJO3CoTC-QnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Remove all envrionment variables*\n",
        "Comes handy in troubleshooting"
      ],
      "metadata": {
        "id": "V1SZw8IDpPWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove all localvariables\n",
        "# ^^^^^^^^^^^^^^^^^^^^^\n",
        "# beg utils\n",
        "# ^^^^^^^^^^^^^^^^^^^^^\n",
        "# local scope\n",
        "myvar = [key for key in locals().keys() if not key.startswith('_')]\n",
        "print (len(locals().keys()))\n",
        "print (len(myvar))\n",
        "# print (myvar)\n",
        "for eachvar in myvar:\n",
        "    print (eachvar)\n",
        "    del locals()[eachvar]\n",
        "print (len(locals().keys()))\n",
        "# global scope\n",
        "myvar = [key for key in globals().keys() if not key.startswith('_')]\n",
        "print (len(globals().keys()))\n",
        "print (len(myvar))\n",
        "# print (myvar)\n",
        "for eachvar in myvar:\n",
        "    print (eachvar)\n",
        "    del globals()[eachvar]\n",
        "print (len(globals().keys()))\n",
        "# ^^^^^^^^^^^^^^^^^^^^^\n",
        "# end utils\n",
        "# ^^^^^^^^^^^^^^^^^^^^^"
      ],
      "metadata": {
        "id": "_Md0_ttShF9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Create Python and Shell envrionment variables*"
      ],
      "metadata": {
        "id": "yBjJx9UWs5lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GCP Project\n",
        "PB_GCP_PROJECT = \"my-project\" #@param {type:\"string\"}\n",
        "\n",
        "# Default values\n",
        "PB_SOLUTION_PREFIX=\"pb_\"  #@param {type:\"string\"}\n",
        "# service account\n",
        "PB_SERVICE_ACCOUNT_NAME=PB_SOLUTION_PREFIX+\"profit-bidder\" #@param {type:\"string\"}\n",
        "PB_SERVICE_ACCOUNT_NAME=PB_SERVICE_ACCOUNT_NAME.replace('_','-')\n",
        "PB_SA_ROLES=\"roles/bigquery.dataViewer roles/pubsub.publisher roles/iam.serviceAccountTokenCreator\"\n",
        "PB_SA_EMAIL=PB_SERVICE_ACCOUNT_NAME + '@' + PB_GCP_PROJECT + '.iam.gserviceaccount.com'\n",
        "\n",
        "# BQ DS for SA360/CM360\n",
        "PB_DS_SA360=PB_SOLUTION_PREFIX + \"sa360_data\" #@param {type:\"string\"}\n",
        "# BQ DS for Business data \n",
        "PB_DS_BUSINESS_DATA=PB_SOLUTION_PREFIX + \"business_data\" #@param {type:\"string\"}\n",
        "# Client margin table\n",
        "PB_CLIENT_MARGIN_DATA_TABLE_NAME=\"client_margin_data_table\" #@param {type:\"string\"}\n",
        "# Tranformed data table\n",
        "PB_CM360_TABLE=\"my_transformed_data\" #@param {type:\"string\"}\n",
        "PB_CM360_PROFILE_ID=\"my_cm_profileid\" #@param {type:\"string\"}\n",
        "PB_CM360_FL_ACTIVITY_ID=\"my_fl_activity_id\" #@param {type:\"string\"}\n",
        "PB_CM360_FL_CONFIG_ID=\"my_fl_config_id\" #@param {type:\"string\"}\n",
        "\n",
        "# DON'T CHNAGE THE BELOW VARIABLES; it is hardcoded to match the test dataset\n",
        "PB_SQL_TRANSFORM_ADVERTISER_ID=\"43939335402485897\" #synthensized id to test.\n",
        "PB_CAMPAIGN_TABLE_NAME=\"p_Campaign_\" + PB_SQL_TRANSFORM_ADVERTISER_ID\n",
        "PB_CONVERSION_TABLE_NAME=\"p_Conversion_\" + PB_SQL_TRANSFORM_ADVERTISER_ID\n",
        "\n",
        "PB_TIMEZONE=\"America/New_York\"\n",
        "\n",
        "PB_REQUIRED_KEYS = [\n",
        "    'conversionId',\n",
        "    'conversionQuantity',\n",
        "    'conversionRevenue',\n",
        "    'conversionTimestamp',\n",
        "    'conversionVisitExternalClickId',\n",
        "]\n",
        "PB_API_SCOPES = ['https://www.googleapis.com/auth/dfareporting',\n",
        "              'https://www.googleapis.com/auth/dfatrafficking',\n",
        "              'https://www.googleapis.com/auth/ddmconversions',\n",
        "              'https://www.googleapis.com/auth/devstorage.read_write']\n",
        "PB_CM360_API_NAME = 'dfareporting'\n",
        "PB_CM360_API_VERSION = 'v3.5'\n",
        "\n",
        "PB_BATCH_SIZE=100\n",
        "\n",
        "# create a variable that you can pass to the bq Cell magic\n",
        "# import the variables to the shell\n",
        "import os\n",
        "PB_all_args = [key for key in locals().keys() if not key.startswith('_')]\n",
        "# print (PB_all_args)\n",
        "PB_BQ_ARGS = {}\n",
        "for PB_each_key in PB_all_args:\n",
        "    # print (f\"{PB_each_key}:{locals()[PB_each_key]}\")\n",
        "    if PB_each_key.upper().startswith(PB_SOLUTION_PREFIX.upper()):\n",
        "      PB_BQ_ARGS[PB_each_key] = locals()[PB_each_key]\n",
        "      os.environ[PB_each_key] = str(PB_BQ_ARGS[PB_each_key])\n",
        "print (PB_BQ_ARGS)"
      ],
      "metadata": {
        "id": "-SVQ3qGx_snb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Setup your Google Cloud project*"
      ],
      "metadata": {
        "id": "U3llp5SbjgRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the desired Google Cloud project\n",
        "!gcloud config set project $PB_GCP_PROJECT\n",
        "import os\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = PB_GCP_PROJECT\n",
        "# validate that the Google Cloud project has been set properly.\n",
        "!echo 'gcloud will use the below project:'\n",
        "!gcloud info --format='value(config.project)'"
      ],
      "metadata": {
        "id": "qvdjQD40je1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Authenticate with Google Cloud*"
      ],
      "metadata": {
        "id": "8H7p8OATBBKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate using ServiceAccount Key file"
      ],
      "metadata": {
        "id": "ro1LOjjeBFbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the ServiceAccount key and provide the path to the file below\n",
        "# PB_GCP_APPLICATION_CREDENTIALS = \"<Full path with the file name to the above downloaded json file>\"\n",
        "# PB_GCP_APPLICATION_CREDENTIALS = \"/Users/dpani/Downloads/dpani-sandbox-2-3073195cd132.json\"\n",
        "\n",
        "# uncomment the below code in codelab environment\n",
        "# authenticate using service account\n",
        "# from google.colab import files\n",
        "# # Upload service account key\n",
        "# keyfile_upload = files.upload()\n",
        "# PB_GCP_APPLICATION_CREDENTIALS = list(keyfile_upload.keys())[0]\n",
        "\n",
        "# import os\n",
        "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = PB_GCP_APPLICATION_CREDENTIALS\n",
        "# # set the account\n",
        "# !echo \"Setting Service Account:\" $PB_GCP_APPLICATION_CREDENTIALS\n",
        "# !gcloud auth activate-service-account --key-file=$PB_GCP_APPLICATION_CREDENTIALS"
      ],
      "metadata": {
        "id": "fRHmtK1oBIhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate using OAuth"
      ],
      "metadata": {
        "id": "87CG0pUMBLhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment the below code in codelab environment\n",
        "# authenticate using oauth\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "f27wF9NOBACx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Enable the below Google Cloud Services for the solution*"
      ],
      "metadata": {
        "id": "qSu_m_AKjEn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the proper Permission for the required Google Cloud Services\n",
        "!gcloud services enable \\\n",
        "  bigquery.googleapis.com \\\n",
        "  bigquerystorage.googleapis.com \\\n",
        "  bigquerydatatransfer.googleapis.com \\\n",
        "  doubleclickbidmanager.googleapis.com \\\n",
        "  doubleclicksearch.googleapis.com \\\n",
        "  storage-api.googleapis.com "
      ],
      "metadata": {
        "id": "ZO9x3hgUjIhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities fuctions"
      ],
      "metadata": {
        "id": "kbQotY4kncTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Delete a dataset in BigQuery (DDL)*"
      ],
      "metadata": {
        "id": "e9BjcT-Znz6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the BigQuery dataset...!!! BE CAREFUL !!!\n",
        "def delete_dataset(dataset_id):\n",
        "    \"\"\"Deletes a BigQuery dataset\n",
        "    This is not recommendated to use it in a production enviornment.\n",
        "    Comes handy in the iterative development and testing phases of the SDLC.\n",
        "    !!! BE CAREFUL !!!!\n",
        "    Args:\n",
        "        dataset_id(:obj:`str`): The BigQuery dataset name that we want to delete\n",
        "    \"\"\"\n",
        "    # [START bigquery_delete_dataset]\n",
        "    from google.cloud import bigquery\n",
        "    # Construct a BigQuery client object.\n",
        "    client = bigquery.Client()\n",
        "    # dataset_id = 'your-project.your_dataset'\n",
        "    # Use the delete_contents parameter to delete a dataset and its contents.\n",
        "    # Use the not_found_ok parameter to not receive an error if the\n",
        "    #     dataset has already been deleted.\n",
        "    client.delete_dataset(\n",
        "        dataset_id, delete_contents=True, not_found_ok=True\n",
        "    )  # Make an API request.\n",
        "    print(\"Deleted dataset '{}'.\".format(dataset_id))"
      ],
      "metadata": {
        "id": "zebhrPLMBfPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Delete a table in BigQuery (DDL)*"
      ],
      "metadata": {
        "id": "wGc5ha8Nn3Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete BigQuery table if not needed...!!! BE CAREFUL !!!\n",
        "def delete_table(table_id):\n",
        "  \"\"\"Deletes a BigQuery table\n",
        "    This is not recommendated to use it in a production enviornment.\n",
        "    Comes handy in the iterative development and testing phases of the SDLC.\n",
        "    !!! BE CAREFUL !!!!\n",
        "    Args:\n",
        "      table_id(:obj:`str`): The BigQuery table name that we want to delete\n",
        "  \"\"\"\n",
        "  from google.cloud import bigquery\n",
        "  # Construct a BigQuery client object.\n",
        "  client = bigquery.Client()\n",
        "  # client.delete_table(table_id, not_found_ok=True)  # Make an API request.\n",
        "  client.delete_table(table_id)  # Make an API request.\n",
        "  print(\"Deleted table '{}'.\".format(table_id))"
      ],
      "metadata": {
        "id": "EYmGT0Iln2Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Deletes a Service Account* "
      ],
      "metadata": {
        "id": "p1MwrM1nn8p1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete a service account\n",
        "def delete_service_account(PB_GCP_PROJECT: str,\n",
        "                 PB_ACCOUNT_NAME: str\n",
        "                 ):\n",
        "  \"\"\"The function deletes a service account\n",
        "\n",
        "    This is not recommendated to use it in a production enviornment.\n",
        "    Comes handy in the iterative development and testing phases of the SDLC.\n",
        "    !!! BE CAREFUL !!!!\n",
        "\n",
        "    Args:\n",
        "      PB_GCP_PROJECT:(:obj:`str`): Google Cloud project for deployment\n",
        "      PB_ACCOUNT_NAME:(:obj:`str`): Name of the service account.\n",
        "  \"\"\"\n",
        "\n",
        "  from googleapiclient import discovery\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "\n",
        "  credentials = GoogleCredentials.get_application_default()\n",
        "\n",
        "  service = discovery.build('iam', 'v1', credentials=credentials)\n",
        "\n",
        "  # The resource name of the service account in the following format:\n",
        "  # `projects/{PROJECT_ID}/serviceAccounts/{ACCOUNT}`.\n",
        "  # Using `-` as a wildcard for the `PROJECT_ID` will infer the project from\n",
        "  # the account. The `ACCOUNT` value can be the `email` address or the\n",
        "  # `unique_id` of the service account.\n",
        "  name = f'projects/{PB_GCP_PROJECT}/serviceAccounts/{PB_ACCOUNT_NAME}@{PB_GCP_PROJECT}.iam.gserviceaccount.com'\n",
        "\n",
        "  print(\"Going to delete service account '{}'.\".format(name))  \n",
        "  request = service.projects().serviceAccounts().delete(name=name)\n",
        "  request.execute() \n",
        "  print(\"Account deleted\")"
      ],
      "metadata": {
        "id": "8MCJx22Jn75D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Profit bid solution"
      ],
      "metadata": {
        "id": "rH7pDQIrm6sc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Creates the Service Account and BigQuery DSs:* \n",
        "*   Service account (the same one used to push the conversion to the SA360/CM360)\n",
        "*   BQ DS for SA360/CM360\n",
        "*   BQ DS for Business data \n"
      ],
      "metadata": {
        "id": "5wNg67lso69b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# create the service account\n",
        "# and add necessary iam roles\n",
        "function get_roles {\n",
        "  gcloud projects get-iam-policy ${PB_GCP_PROJECT} --flatten=\"bindings[].members\" --format='table(bindings.role)' --filter=\"bindings.members:${PB_SA_EMAIL}\"\n",
        "}\n",
        "function create_service_account {\n",
        "  echo \"Creating service account $PB_SA_EMAIL\"\n",
        "  gcloud iam service-accounts describe $PB_SA_EMAIL > /dev/null 2>&1\n",
        "  RETVAL=$?\n",
        "  if (( ${RETVAL} != \"0\" )); then\n",
        "    gcloud iam service-accounts create ${PB_SERVICE_ACCOUNT_NAME} --description 'Profit Bidder Service Account' --project ${PB_GCP_PROJECT}\n",
        "  fi\n",
        "  for role in ${PB_SA_ROLES}; do\n",
        "    echo -n \"Adding ${PB_SERVICE_ACCOUNT_NAME} to ${role} \"\n",
        "    if get_roles | grep $role &> /dev/null; then\n",
        "      echo \"already added.\"\n",
        "    else\n",
        "      gcloud projects add-iam-policy-binding ${PB_GCP_PROJECT} --member=\"serviceAccount:${PB_SA_EMAIL}\" --role=\"${role}\"\n",
        "      echo \"added.\"\n",
        "    fi\n",
        "  done   \n",
        "}\n",
        "# Creates the service account and adds necessary permissions\n",
        "create_service_account\n",
        "\n",
        "function create_bq_ds {\n",
        "  dataset=$1\n",
        "  echo \"Creating BQ dataset: '${dataset}'\" \n",
        "  bq --project_id=${PB_GCP_PROJECT} show --dataset ${dataset} > /dev/null 2>&1\n",
        "  RETVAL=$?\n",
        "  if (( ${RETVAL} != \"0\" )); then\n",
        "    bq --project_id=${PB_GCP_PROJECT} mk --dataset ${dataset}\n",
        "  else\n",
        "    echo \"Reusing ${dataset}.\"\n",
        "  fi\n",
        "}\n",
        "#create the BQ DSs\n",
        "create_bq_ds $PB_DS_SA360\n",
        "create_bq_ds $PB_DS_BUSINESS_DATA"
      ],
      "metadata": {
        "id": "ZStdYWN1oIQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Download the test data*\n",
        "Test data is in 'solution_test' folder"
      ],
      "metadata": {
        "id": "DDsdt3g9kAEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Download the test data from gitrepo\n",
        "DIR=$HOME/solutions/profit-bidder\n",
        "if [ -d \"$DIR\" ]\n",
        "then\n",
        "  echo $DIR already exists.\n",
        "else\n",
        "  mkdir -p $HOME/solutions/profit-bidder\n",
        "  cd $HOME/solutions/profit-bidder\n",
        "  git clone https://github.com/google/profit-bidder.git .\n",
        "fi\n",
        "export PB_TEST_DATA_DIR=$DIR/solution_test\n",
        "ls -ltrah $PB_TEST_DATA_DIR\n",
        "echo $PB_TEST_DATA_DIR folder contains the test data."
      ],
      "metadata": {
        "id": "qJ8CbEp6B2hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Uploads Test data to BigQuery* "
      ],
      "metadata": {
        "id": "XLdb4vso6ITZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# uploades the test data into the BigQuery\n",
        "function create_bq_table {\n",
        "  dataset=$1\n",
        "  table_name=$2\n",
        "  schema_name=$3\n",
        "\n",
        "  sql_result=$(list_bq_table $1 $2)\n",
        "  echo \"Creating BQ table: '${dataset}.${table_name}'\" \n",
        "  if [[ \"$sql_result\" == *\"1\"* ]]; then\n",
        "    echo \"Reusing ${dataset}.${table_name}.\"\n",
        "  else\n",
        "    bq --project_id=${PB_GCP_PROJECT} mk -t --schema ${schema_name} --time_partitioning_type DAY ${dataset}.${table_name}\n",
        "  fi  \n",
        "}\n",
        "\n",
        "function delete_bq_table {\n",
        "  dataset=$1\n",
        "  table_name=$2\n",
        "  sql_result=$(list_bq_table $1 $2)\n",
        "  echo \"Deleting BQ table: '${dataset}.${table_name}'\" \n",
        "  if [[ \"$sql_result\" == *\"1\"* ]]; then\n",
        "    bq rm -f -t $PB_GCP_PROJECT:$dataset.$table_name\n",
        "  else\n",
        "    echo \"${dataset}.${table_name} doesn't exists.\"\n",
        "  fi  \n",
        "}\n",
        "\n",
        "function list_bq_table {\n",
        "  dataset=$1\n",
        "  table_name=$2\n",
        "  echo \"Checking BQ table exist: '${dataset}.${table_name}'\" \n",
        "  sql_query='SELECT\n",
        "    COUNT(1) AS cnt\n",
        "  FROM \n",
        "    `<myproject>`.<mydataset>.__TABLES_SUMMARY__\n",
        "  WHERE table_id = \"<mytable_name>\"'\n",
        "  sql_query=\"${sql_query/<myproject>/${PB_GCP_PROJECT}}\"\n",
        "  sql_query=\"${sql_query/<mydataset>/${dataset}}\"\n",
        "  sql_query=\"${sql_query/<mytable_name>/${table_name}}\"\n",
        "\n",
        "  bq_qry_cmd=\"bq query --use_legacy_sql=false --format=csv '<mysql_qery>'\"\n",
        "  bq_qry_cmd=\"${bq_qry_cmd/<mysql_qery>/${sql_query}}\"\n",
        "  sql_result=$(eval $bq_qry_cmd)  \n",
        "  if [[ \"$sql_result\" == *\"1\"* ]]; then\n",
        "    echo \"${dataset}.${table_name} exist\"\n",
        "    echo \"1\"\n",
        "  else\n",
        "    echo \"${dataset}.${table_name} doesn't exist\"\n",
        "    echo \"0\"\n",
        "  fi   \n",
        "}\n",
        "\n",
        "function load_bq_table {\n",
        "  dataset=$1\n",
        "  table_name=$2\n",
        "  data_file=$3\n",
        "  schema_name=$4\n",
        "  sql_result=$(list_bq_table $1 $2)\n",
        "  echo \"Loading data to BQ table: '${dataset}.${table_name}'\" \n",
        "  if [[ \"$sql_result\" == *\"1\"* ]]; then\n",
        "    delete_bq_table $dataset $table_name\n",
        "  fi  \n",
        "  if [[ \"$schema_name\" == *\"autodetect\"* ]]; then\n",
        "    bq --project_id=${PB_GCP_PROJECT} load \\\n",
        "    --autodetect \\\n",
        "    --source_format=CSV \\\n",
        "    $dataset.$table_name \\\n",
        "    $data_file \n",
        "  else\n",
        "    create_bq_table $dataset $table_name $schema_name\n",
        "    bq --project_id=${PB_GCP_PROJECT} load \\\n",
        "      --source_format=CSV \\\n",
        "      --time_partitioning_type=DAY \\\n",
        "      --skip_leading_rows=1 \\\n",
        "      ${dataset}.${table_name} \\\n",
        "      ${data_file}\n",
        "  fi  \n",
        "}\n",
        "\n",
        "# save the current working dierctory\n",
        "current_working_dir=`pwd`\n",
        "\n",
        "# change to the test data directory\n",
        "DIR=$HOME/solutions/profit-bidder\n",
        "export PB_TEST_DATA_DIR=$DIR/solution_test\n",
        "ls -ltrah $PB_TEST_DATA_DIR\n",
        "echo $PB_TEST_DATA_DIR folder contains the test data.\n",
        "cd $PB_TEST_DATA_DIR\n",
        "pwd\n",
        "\n",
        "# create campaign table\n",
        "# load test data to campaign table\n",
        "load_bq_table $PB_DS_SA360 $PB_CAMPAIGN_TABLE_NAME \"p_Campaign_${PB_SQL_TRANSFORM_ADVERTISER_ID}.csv\" \"p_Campaign_schema.json\"\n",
        "# create conversion table\n",
        "# load test data to conversion\n",
        "load_bq_table $PB_DS_SA360 $PB_CONVERSION_TABLE_NAME \"p_Conversion_${PB_SQL_TRANSFORM_ADVERTISER_ID}.csv\" \"${PB_TEST_DATA_DIR}/p_Conversion_schema.json\"\n",
        "# load test profit data\n",
        "load_bq_table $PB_DS_BUSINESS_DATA $PB_CLIENT_MARGIN_DATA_TABLE_NAME \"client_profit.csv\" \"autodetect\"\n",
        "\n",
        "# change to original working directory\n",
        "cd $current_working_dir\n",
        "pwd\n",
        "\n"
      ],
      "metadata": {
        "id": "-LFxbRg-6JFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Create a BigQuery client, import the libraries, load the bigquery Cell magic*"
      ],
      "metadata": {
        "id": "CeNUMorFRweS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a BigQuery client\n",
        "from google.cloud import bigquery\n",
        "bq_client = bigquery.Client(project=PB_GCP_PROJECT)\n",
        "# load the bigquery Cell magic\n",
        "# %load_ext google.cloud.bigquery\n",
        "%reload_ext google.cloud.bigquery"
      ],
      "metadata": {
        "id": "H58XhkfDRyNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test that BigQuery client works\n",
        "sql = \"\"\"\n",
        "    SELECT name\n",
        "    FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
        "    WHERE state = 'TX'\n",
        "    LIMIT 100\n",
        "\"\"\"\n",
        "\n",
        "# Run a Standard SQL query using the environment's default project\n",
        "df = bq_client.query(sql).to_dataframe()\n",
        "df"
      ],
      "metadata": {
        "id": "I4JSFrtrR58K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Transform and aggregate* "
      ],
      "metadata": {
        "id": "WC9juyBaMvsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The below query transforms the data from Campaign, Conversion, \n",
        "#   and profit tables.\n",
        "aggregate_sql = f\"\"\"\n",
        "-- Copyright 2021 Google LLC\n",
        "--\n",
        "-- Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "-- you may not use this file except in compliance with the License.\n",
        "-- You may obtain a copy of the License at\n",
        "--\n",
        "--      http://www.apache.org/licenses/LICENSE-2.0\n",
        "--\n",
        "-- Unless required by applicable law or agreed to in writing, software\n",
        "-- distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "-- See the License for the specific language governing permissions and\n",
        "-- limitations under the License.\n",
        "\n",
        "-- ******    TEMPLATE CODE    ******\n",
        "-- NOTE: Please thoroughly review and test your version of this query before launching your pipeline\n",
        "-- The resulting data from this script should provide all the necessary columns for upload via \n",
        "-- the CM360 API and the SA360 API\n",
        "\n",
        "-- \n",
        "-- the below placeholders must be replaced with appropriate values.\n",
        "--      install.sh does so\n",
        "-- project_id as: {PB_GCP_PROJECT}\n",
        "-- sa360_dataset_name as: {PB_DS_SA360}\n",
        "-- advertiser_id as: {PB_SQL_TRANSFORM_ADVERTISER_ID}\n",
        "-- timezone as: America/New_York e.g. America/New_York\n",
        "-- floodlight_name as: My Sample Floodlight Activity\n",
        "-- account_type as: Other engines\n",
        "-- gmc_dataset_name as: pb_gmc_data\n",
        "-- gmc_account_id as: mygmc_account_id\n",
        "-- business_dataset_name as: {PB_DS_BUSINESS_DATA}\n",
        "-- client_margin_data_table as: {PB_CLIENT_MARGIN_DATA_TABLE_NAME}\n",
        "-- client_profit_data_sku_col as: sku\n",
        "-- client_profit_data_profit_col as: profit\n",
        "-- target_floodlight_name as: My Sample Floodlight Activity\n",
        "-- product_sku_var as: u9\n",
        "-- product_quantity_var as: u10\n",
        "-- product_unit_price_var as: u11\n",
        "-- product_sku_regex as: (.*?);\n",
        "-- product_quantity_regex as: (.*?);\n",
        "-- product_unit_price_regex as: (.*?);\n",
        "-- product_sku_delim as: |\n",
        "-- product_quantity_delim as: |\n",
        "-- product_unit_price_delim as: |\n",
        "-- \n",
        "\n",
        "WITH\n",
        "campaigns AS (\n",
        "    -- Example: Extracting all campaign names and IDs if needed for filtering for\n",
        "    -- conversions for a subset of campaigns\n",
        "    SELECT\n",
        "        campaign,\n",
        "        campaignId,\n",
        "        row_number() OVER (partition BY campaignId ORDER BY lastModifiedTimestamp DESC) as row_num -- for de-duping\n",
        "    FROM `{PB_GCP_PROJECT}.{PB_DS_SA360}.p_Campaign_{PB_SQL_TRANSFORM_ADVERTISER_ID}`\n",
        "    -- Be sure to replace the Timezone with what is appropriate for your use case\n",
        "    WHERE EXTRACT(DATE FROM _PARTITIONTIME) >= DATE_SUB(CURRENT_DATE('America/New_York'), INTERVAL 7 DAY)\n",
        ")\n",
        ",expanded_conversions AS (\n",
        "    -- Parses out all relevant product data from a conversion request string\n",
        "    SELECT\n",
        "        conv.*,\n",
        "        campaign,\n",
        "        -- example of U-Variables that are parsed to extract product purchase data\n",
        "        SPLIT(REGEXP_EXTRACT(floodlightEventRequestString, \"u9=(.*?);\"),\"|\") AS u9,\n",
        "        SPLIT(REGEXP_EXTRACT(floodlightEventRequestString, \"u10=(.*?);\"),\"|\") AS u10,\n",
        "        SPLIT(REGEXP_EXTRACT(floodlightEventRequestString, \"u11=(.*?);\"),\"|\") AS u11,\n",
        "    FROM `{PB_GCP_PROJECT}.{PB_DS_SA360}.p_Conversion_{PB_SQL_TRANSFORM_ADVERTISER_ID}` AS conv\n",
        "    LEFT JOIN (\n",
        "        SELECT campaign, campaignId\n",
        "        FROM campaigns\n",
        "        WHERE row_num = 1\n",
        "        GROUP BY 1,2\n",
        "    ) AS camp\n",
        "    USING (campaignId)\n",
        "    WHERE\n",
        "        -- Filter for conversions that occured in the previous day\n",
        "        -- Be sure to replace the Timezone with what is appropriate for your use case\n",
        "        floodlightActivity IN ('My Sample Floodlight Activity')\n",
        "        AND accountType = 'Other engines' -- filter by Account Type as needed\n",
        ")\n",
        ",flattened_conversions AS (\n",
        "    -- Flattens the extracted product data for each conversion which leaves us with a row\n",
        "    -- of data for each product purchased as part of a given conversion\n",
        "    SELECT\n",
        "        advertiserId,\n",
        "        campaignId,\n",
        "        conversionId,\n",
        "        skuId,\n",
        "        pos1,\n",
        "        quantity,\n",
        "        pos2,\n",
        "        cost,\n",
        "        pos3\n",
        "    FROM expanded_conversions,\n",
        "    UNNEST(expanded_conversions.u9) AS skuId WITH OFFSET pos1,\n",
        "    UNNEST(expanded_conversions.u10) AS quantity WITH OFFSET pos2,\n",
        "    UNNEST(expanded_conversions.u11) AS cost WITH OFFSET pos3\n",
        "    WHERE pos1 = pos2 AND pos1 = pos3 AND skuId != ''\n",
        "    GROUP BY 1,2,3,4,5,6,7,8,9\n",
        "    ORDER BY conversionId\n",
        ")\n",
        ",inject_gmc_margin AS (\n",
        "    -- Merges Margin data with the products found in the conversion data\n",
        "    SELECT \n",
        "        advertiserId,\n",
        "        campaignId,\n",
        "        conversionId,\n",
        "        skuId,\n",
        "        quantity,\n",
        "        IF(cost = '', '0', cost) as cost,\n",
        "        pos1,\n",
        "        pos2,\n",
        "        pos3,\n",
        "        -- PLACEHOLDER MARGIN, X% for unclassified items\n",
        "        CASE\n",
        "        WHEN profit IS NULL THEN 0.0\n",
        "        ELSE profit\n",
        "        END AS margin,\n",
        "        sku,\n",
        "    FROM flattened_conversions\n",
        "    LEFT JOIN `{PB_GCP_PROJECT}.{PB_DS_BUSINESS_DATA}.{PB_CLIENT_MARGIN_DATA_TABLE_NAME}`\n",
        "    ON flattened_conversions.skuId = sku\n",
        "group by 1,2,3,4,5,6,7,8,9,10,11\n",
        ")\n",
        ",all_conversions as (\n",
        "    -- Rolls up all previously expanded conversion data while calculating profit based on the matched \n",
        "    -- margin value. Also assigns timestamp in millis and micros \n",
        "    SELECT\n",
        "        e.account,\n",
        "        e.accountId,\n",
        "        e.accountType,\n",
        "        e.advertiser,\n",
        "        igm.advertiserId,\n",
        "        e.agency,\n",
        "        e.agencyId,\n",
        "        igm.campaignId,\n",
        "        e.campaign,\n",
        "        e.conversionAttributionType,\n",
        "        e.conversionDate,\n",
        "        -- '00' may be changed to any string value that will help you identify these\n",
        "        -- new conversions in reporting\n",
        "        CONCAT(igm.conversionId, '00') as conversionId,\n",
        "        e.conversionLastModifiedTimestamp,\n",
        "        -- Note:Rounds float quantity and casts to INT, change based on use case\n",
        "        -- This is done to support CM360 API\n",
        "        CAST(ROUND(e.conversionQuantity) AS INT64) AS conversionQuantity,\n",
        "        e.conversionRevenue,\n",
        "        SUM(\n",
        "            FLOOR(CAST(igm.cost AS FLOAT64))\n",
        "        ) AS CALCULATED_REVENUE,\n",
        "        -- PROFIT CALCULATED HERE, ADJUST LOGIC AS NEEDED FOR YOUR USE CASE\n",
        "        ROUND(\n",
        "            SUM(\n",
        "                -- multiply item cost by class margin\n",
        "                SAFE_MULTIPLY(\n",
        "                    CAST(igm.cost AS FLOAT64),\n",
        "                    igm.margin)\n",
        "            ),2\n",
        "        ) AS CALCULATED_PROFIT,\n",
        "        e.conversionSearchTerm,\n",
        "        e.conversionTimestamp,\n",
        "        -- SA360 timestamp should be in millis\n",
        "        UNIX_MILLIS(e.conversionTimestamp) as conversionTimestampMillis,\n",
        "        -- CM360 Timestamp should be in micros\n",
        "        UNIX_MICROS(e.conversionTimestamp) as conversionTimestampMicros,\n",
        "        e.conversionType,\n",
        "        e.conversionVisitExternalClickId,\n",
        "        e.conversionVisitId,\n",
        "        e.conversionVisitTimestamp,\n",
        "        e.deviceSegment,\n",
        "        e.floodlightActivity,\n",
        "        e.floodlightActivityId,\n",
        "        e.floodlightActivityTag,\n",
        "        e.floodlightEventRequestString,\n",
        "        e.floodlightOrderId,\n",
        "        e.floodlightOriginalRevenue,\n",
        "        status\n",
        "    FROM inject_gmc_margin AS igm\n",
        "    LEFT JOIN expanded_conversions AS e\n",
        "    ON igm.advertiserID = e.advertiserId AND igm.campaignId = e.campaignID AND igm.conversionId = e.conversionId\n",
        "    GROUP BY 1,2,3,4,5,6,8,7,9,10,11,12,13,14,15,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33\n",
        ")\n",
        "-- The columns below represent the original conversion data with their new profit\n",
        "-- values calculated (assigned to conversionRevenue column) along with any original \n",
        "-- floofdlight data that the client wishes to keep for trouble shooting.\n",
        "SELECT \n",
        "    account,\n",
        "    accountId,\n",
        "    accountType,\n",
        "    advertiser,\n",
        "    advertiserId,\n",
        "    agency,\n",
        "    agencyId,\n",
        "    campaignId,\n",
        "    campaign,\n",
        "    conversionId,\n",
        "    conversionAttributionType,\n",
        "    conversionDate,\n",
        "    conversionTimestamp,\n",
        "    conversionTimestampMillis,\n",
        "    conversionTimestampMicros,\n",
        "    CALCULATED_PROFIT AS conversionRevenue,\n",
        "    conversionQuantity,\n",
        "    -- The below is used only troublehsooting purpose.\n",
        "    \"My Sample Floodlight Activity\" AS floodlightActivity,\n",
        "    conversionSearchTerm,\n",
        "    conversionType,\n",
        "    conversionVisitExternalClickId,\n",
        "    conversionVisitId,\n",
        "    conversionVisitTimestamp,\n",
        "    deviceSegment,\n",
        "    CALCULATED_PROFIT,\n",
        "    CALCULATED_REVENUE,\n",
        "    -- Please prefix any original conversion values you wish to keep with \"original\". \n",
        "    -- These values may help with troubleshooting\n",
        "    conversionRevenue AS originalConversionRevenue,\n",
        "    floodlightActivity AS originalFloodlightActivity,\n",
        "    floodlightActivityId AS originalFloodlightActivityId,\n",
        "    floodlightActivityTag AS originalFloodlightActivityTag,\n",
        "    floodlightOriginalRevenue AS originalFloodlightRevenue,\n",
        "    floodlightEventRequestString,\n",
        "    floodlightOrderId\n",
        "FROM all_conversions\n",
        "WHERE CALCULATED_PROFIT > 0.0\n",
        "ORDER BY account ASC\n",
        "\"\"\"\n",
        "# execute the transform query \n",
        "df = bq_client.query(aggregate_sql).to_dataframe()\n",
        "# print a couple of records of the transformed query\n",
        "df.head()"
      ],
      "metadata": {
        "id": "fb5trCwwMu_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the data to a table\n",
        "df.to_gbq(f'{PB_DS_BUSINESS_DATA}.{PB_CM360_TABLE}', \n",
        "          project_id=PB_GCP_PROJECT,\n",
        "          if_exists='replace', \n",
        "          progress_bar=True,)"
      ],
      "metadata": {
        "id": "DozaTyFlTpRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Formulate the payload and push to CM360* "
      ],
      "metadata": {
        "id": "UeBG5A84dQFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reads the from transformed table, chunks the data, \n",
        "#   and uploads the data to CM360\n",
        "# We need to chunk the data so as to adhere \n",
        "#   to the payload limit of the CM360 REST API.\n",
        "import pytz\n",
        "import datetime\n",
        "import decimal\n",
        "import logging\n",
        "import json\n",
        "import google.auth\n",
        "import google.auth.impersonated_credentials\n",
        "import google_auth_httplib2\n",
        "from googleapiclient import discovery\n",
        "\n",
        "def today_date(timezone):\n",
        "    \"\"\"Returns today's date using the timezone\n",
        "    Args:\n",
        "        timezone(:obj:`str`): The timezone with default to America/New_York\n",
        "    Returns:\n",
        "      Date: today's date\n",
        "    \"\"\"\n",
        "    tz = pytz.timezone(timezone)\n",
        "    return datetime.datetime.now(tz).date()\n",
        "\n",
        "def time_now_str(timezone):\n",
        "    \"\"\"Returns today's date using the timezone\n",
        "    Args:\n",
        "        timezone(:obj:`str`): The timezone with default to America/New_York\n",
        "    Returns:\n",
        "      Timezone: current timezone\n",
        "    \"\"\"\n",
        "    # set correct timezone for datetime check\n",
        "    tz = pytz.timezone(timezone)\n",
        "    return datetime.datetime.now(tz).strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
        "\n",
        "def pluralize(count):\n",
        "    \"\"\"An utility function \n",
        "    Args:\n",
        "        count(:obj:`int`): A number\n",
        "    Returns:\n",
        "      str: 's' or empty\n",
        "    \"\"\"\n",
        "    if count > 1:\n",
        "        return 's'\n",
        "    return ''  \n",
        "\n",
        "def get_data(table_ref_name, cloud_client, batch_size):\n",
        "    \"\"\"Returns the data from the transformed table.\n",
        "    Args:\n",
        "        table_ref_name(:obj:`google.cloud.bigquery.table.Table`): Reference to the table\n",
        "        cloud_client(:obj:`google.cloud.bigquery.client.Client`): BigQuery client\n",
        "        batch_size(:obj:`int`): Batch size\n",
        "    Returns:\n",
        "      Array[]: list/rows of data\n",
        "    \"\"\"\n",
        "\n",
        "    current_batch = []\n",
        "    table = cloud_client.get_table(table_ref_name)\n",
        "    print(f'Downloading {table.num_rows} rows from table {table_ref_name}')\n",
        "    skip_stats = {}\n",
        "    for row in cloud_client.list_rows(table_ref_name):\n",
        "        missing_keys = []\n",
        "        for key in PB_REQUIRED_KEYS:\n",
        "            val = row.get(key)\n",
        "            if val is None:\n",
        "                missing_keys.append(key)\n",
        "                count = skip_stats.get(key, 0)\n",
        "                count += 1\n",
        "                skip_stats[key] = count\n",
        "        if len(missing_keys) > 0:\n",
        "            row_as_dict = dict(row.items())\n",
        "            logging.debug(f'Skipped row: missing values for keys {missing_keys} in row {row_as_dict}')\n",
        "            continue\n",
        "        result = {}\n",
        "        conversionTimestamp = row.get('conversionTimestamp')\n",
        "        # convert floating point seconds to microseconds since the epoch\n",
        "        result['conversionTimestampMicros'] = int(conversionTimestamp.timestamp() * 1_000_000)\n",
        "        for key in row.keys():\n",
        "            value = row.get(key)\n",
        "            if type(value) == datetime.datetime or type(value) == datetime.date:\n",
        "                result[key] = value.strftime(\"%y-%m-%d \")\n",
        "            elif type(value) == decimal.Decimal:\n",
        "                result[key] = float(value)\n",
        "            else:\n",
        "                result[key] = value\n",
        "        current_batch.append(result)\n",
        "        if len(current_batch) >= batch_size:\n",
        "            yield current_batch\n",
        "            current_batch = []\n",
        "    if len(current_batch) > 0:\n",
        "        yield current_batch\n",
        "    pretty_skip_stats = ', '.join([f'{val} row{pluralize(val)} missing key \"{key}\"' for key, val in skip_stats.items()])\n",
        "    logging.info(f'Processed {table.num_rows} from table {table_ref_name} skipped {pretty_skip_stats}')\n",
        "\n",
        "def setup(sa_email, api_scopes, api_name, api_version):\n",
        "    \"\"\"Impersonates a service account, authenticate with Google Service,\n",
        "      and returns a discovery api for further communication with Google Services.\n",
        "    Args:\n",
        "        sa_email(:obj:`str`): Service Account to impersonate\n",
        "        api_scopes(:obj:`Any`): An array of scope that the service account \n",
        "          expectes to have permission in the CM360\n",
        "        api_name(:obj:`str`): CM360 API Name\n",
        "        api_version(:obj:`str`): CM360 API version\n",
        "    Returns:\n",
        "      module:discovery: to interact with Goolge Services.\n",
        "    \"\"\"\n",
        "\n",
        "    source_credentials, project_id = google.auth.default()\n",
        "\n",
        "    target_credentials = google.auth.impersonated_credentials.Credentials(\n",
        "        source_credentials=source_credentials,\n",
        "        target_principal=sa_email,\n",
        "        target_scopes=api_scopes,\n",
        "        delegates=[],\n",
        "        lifetime=500)\n",
        "\n",
        "    http = google_auth_httplib2.AuthorizedHttp(target_credentials)\n",
        "    # setup API service here\n",
        "    try: \n",
        "      return discovery.build(\n",
        "          api_name,\n",
        "          api_version,\n",
        "          cache_discovery=False,\n",
        "          http=http)\n",
        "    except:\n",
        "        print('Could not authenticate')    \n",
        "\n",
        "\n",
        "def upload_data(timezone, rows, profile_id, fl_configuration_id, fl_activity_id):\n",
        "    \"\"\"POSTs the conversion data using CM360 API\n",
        "    Args:\n",
        "        timezone(:obj:`Timezone`): Current timezone or defaulted to America/New_York \n",
        "        rows(:obj:`Any`): An array of conversion data\n",
        "        profile_id(:obj:`str`): Profile id - should be gathered from the CM360\n",
        "        fl_configuration_id(:obj:`str`): Floodlight config id - should be gathered from the CM360\n",
        "        fl_activity_id(:obj:`str`): Floodlight activity id - should be gathered from the CM360\n",
        "    \"\"\"\n",
        "  \n",
        "    print('Starting conversions for ' + time_now_str(timezone))\n",
        "    if not fl_activity_id or not fl_configuration_id:\n",
        "        print('Please make sure to provide a value for both floodlightActivityId and floodlightConfigurationId!!')\n",
        "        return\n",
        "    # Build the API connection\n",
        "    try:       \n",
        "      service = setup(PB_SA_EMAIL, PB_API_SCOPES, \n",
        "                      PB_CM360_API_NAME,  PB_CM360_API_VERSION)\n",
        "      # upload_log = ''\n",
        "      print('Authorization successful')\n",
        "      currentrow = 0\n",
        "      all_conversions = \"\"\"{\"kind\": \"dfareporting#conversionsBatchInsertRequest\", \"conversions\": [\"\"\"\n",
        "      while currentrow < len(rows):\n",
        "          for row in rows[currentrow:min(currentrow+100, len(rows))]:\n",
        "              conversion = json.dumps({\n",
        "                  'kind': 'dfareporting#conversion',\n",
        "                  'gclid': row['conversionVisitExternalClickId'],\n",
        "                  'floodlightActivityId': fl_activity_id, # (Use short form CM Floodlight Activity Id )\n",
        "                  'floodlightConfigurationId': fl_configuration_id, # (Can be found in CM UI)\n",
        "                  'ordinal': row['conversionId'],\n",
        "                  'timestampMicros': row['conversionTimestampMicros'],\n",
        "                  'value': row['conversionRevenue'],\n",
        "                  'quantity': row['conversionQuantity'] #(Alternatively, this can be hardcoded to 1)\n",
        "              })\n",
        "              # print('Conversion: ', conversion) # uncomment if you want to output each conversion\n",
        "              all_conversions = all_conversions + conversion + ','\n",
        "          all_conversions = all_conversions[:-1] + ']}'\n",
        "          payload = json.loads(all_conversions)\n",
        "          print(f'CM360 request payload: {payload}')\n",
        "          request = service.conversions().batchinsert(profileId=profile_id, body=payload)\n",
        "          print('[{}] - CM360 API Request: '.format(time_now_str()), request)\n",
        "          response = request.execute()\n",
        "          print('[{}] - CM360 API Response: '.format(time_now_str()), response)\n",
        "          if not response['hasFailures']:\n",
        "              print('Successfully inserted batch of 100.')\n",
        "          else:\n",
        "              status = response['status']\n",
        "              for line in status:\n",
        "                  try:\n",
        "                      if line['errors']:\n",
        "                          for error in line['errors']:\n",
        "                              print('Error in line ' + json.dumps(line['conversion']))\n",
        "                              print('\\t[%s]: %s' % (error['code'], error['message']))\n",
        "                  except:\n",
        "                      print('Conversion with gclid ' + line['gclid'] + ' inserted.')\n",
        "          print('Either finished or found errors.')\n",
        "          currentrow += 100\n",
        "          all_conversions = \"\"\"{\"kind\": \"dfareporting#conversionsBatchInsertRequest\", \"conversions\": [\"\"\"\n",
        "    except:\n",
        "        print('Could not authenticate')    \n",
        "\n",
        "def partition_and_distribute(cloud_client, table_ref_name, batch_size, timezone, \n",
        "                             profile_id, fl_configuration_id, fl_activity_id):\n",
        "    \"\"\"Partitions the data to chunks of batch size and\n",
        "        uploads to the CM360\n",
        "    Args:\n",
        "        table_ref_name(:obj:`google.cloud.bigquery.table.Table`): Reference to the table\n",
        "        cloud_client(:obj:`google.cloud.bigquery.client.Client`): BigQuery client\n",
        "        batch_size(:obj:`int`): Batch size\n",
        "        timezone(:obj:`Timezone`): Current timezone or defaulted to America/New_York \n",
        "        profile_id(:obj:`str`): Profile id - should be gathered from the CM360\n",
        "        fl_configuration_id(:obj:`str`): Floodlight config id - should be gathered from the CM360\n",
        "        fl_activity_id(:obj:`str`): Floodlight activity id - should be gathered from the CM360\n",
        "    \"\"\"\n",
        "    for batch in get_data(table_ref_name, cloud_client, batch_size):\n",
        "        # print(f'Batch size: {len(batch)} batch: {batch}')\n",
        "        upload_data(timezone, batch, profile_id, fl_configuration_id, \n",
        "                    fl_activity_id)\n",
        "        # DEBUG BREAK!\n",
        "        if batch_size == 1:\n",
        "            break\n",
        "\n",
        "try: \n",
        "    table = bq_client.get_table(f'{PB_DS_BUSINESS_DATA}.{PB_CM360_TABLE}')\n",
        "except:\n",
        "    print ('Could not find table with the provided table name: {}.'.format(f'{PB_DS_BUSINESS_DATA}.{PB_CM360_TABLE}'))    \n",
        "    table = None\n",
        "\n",
        "todays_date = today_date(PB_TIMEZONE)\n",
        "\n",
        "if table is not None:\n",
        "    table_ref_name = table.full_table_id.replace(':', '.')\n",
        "    if table.modified.date() == todays_date or table.created.date() == todays_date:\n",
        "        print('[{}] is up-to-date. Continuing with upload...'.format(table_ref_name))\n",
        "        partition_and_distribute(bq_client, table_ref_name, PB_BATCH_SIZE,\n",
        "                                 PB_TIMEZONE, PB_CM360_PROFILE_ID, \n",
        "                                 PB_CM360_FL_CONFIG_ID, PB_CM360_FL_ACTIVITY_ID) \n",
        "    else:\n",
        "        print('[{}] data may be stale. Please check workflow to verfiy that it has run correctly. Upload is aborted!'.format(table_ref_name))\n",
        "else:\n",
        "    print('Table not found! Please double check your workflow for any errors.')"
      ],
      "metadata": {
        "id": "6i7DEHwnTbGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean up - !!! BE CAREFUL!!!"
      ],
      "metadata": {
        "id": "ViUCLkF1B9R2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete the transformed table"
      ],
      "metadata": {
        "id": "MNMy4N9pb4sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deletes the transformed table\n",
        "delete_table(f'{PB_DS_BUSINESS_DATA}.{PB_CM360_TABLE}')"
      ],
      "metadata": {
        "id": "H9hM4NtVb-lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete the SA and BQ DSs:\n",
        "*   Service account (the same one used to push the conversion to the SA360/CM360)\n",
        "*   BQ DS for SA360/CM360\n",
        "*   BQ DS for Business data \n"
      ],
      "metadata": {
        "id": "t_Uoy1upCDDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deletes the service account\n",
        "delete_service_account(PB_GCP_PROJECT, PB_SERVICE_ACCOUNT_NAME)\n",
        "# deletes the dataset\n",
        "delete_dataset(PB_DS_SA360)\n",
        "delete_dataset(PB_DS_BUSINESS_DATA)"
      ],
      "metadata": {
        "id": "NMmtznD0CGOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete the Google Cloud Project\n",
        "To avoid incurring charges to your Google Cloud Platform account for the resources used in this tutorial is to **Delete the project**.\n",
        "\n",
        "The easiest way to eliminate billing is to delete the project you created for the tutorial.\n",
        "\n",
        "**Caution**: Deleting a project has the following effects:\n",
        "* *Everything in the project is deleted.* If you used an existing project for this tutorial, when you delete it, you also delete any other work you've done in the project.\n",
        "* <b>Custom project IDs are lost. </b>When you created this project, you might have created a custom project ID that you want to use in the future. To preserve the URLs that use the project ID, such as an appspot.com</b> URL, delete selected resources inside the project instead of deleting the whole project. \n",
        "\n",
        "If you plan to explore multiple tutorials and quickstarts, reusing projects can help you avoid exceeding project quota limits.\n",
        "<br>\n",
        "<ol type=\"1\">\n",
        "    <li>In the Cloud Console, go to the <b>Manage resources</b> page.</li>\n",
        "    Go to the <a href=\"https://console.cloud.google.com/iam-admin/projects\">Manage resources page</a>\n",
        "    <li>In the project list, select the project that you want to delete and then click <b>Delete</b> Trash icon.</li>\n",
        "    <li>In the dialog, type the project ID and then click <b>Shut down</b> to delete the project. </li>\n",
        "</ol>\n"
      ],
      "metadata": {
        "id": "asaWrFYsCh2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jZ5ImDq5Ca0_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}